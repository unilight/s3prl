runner:
  total_steps: 10000
  gradient_clipping: 1
  gradient_accumulate_steps: 1

  log_step: 100
  eval_step: 1000
  save_step: 1000
  #log_step: 10
  #eval_step: 50
  #save_step: 50
  max_keep: 10
  eval_dataloaders:
    - dev
    - test

optimizer:
  name: AdamW
  lr: 1.0e-4

# comment the whole scheduler config block
# to disable learning rate scheduling
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 4000

downstream_expert:
  datarc:
    num_workers: 3
    train_batch_size: 6
    eval_batch_size: 5
    
    data_root: "./downstream/a2o-vc-vcc2020/data/vcc2020"
    lists_root: "./downstream/a2o-vc-vcc2020/data/lists"
    stats_root: "./downstream/a2o-vc-vcc2020/data/stats"
    f0_path: "./downstream/a2o-vc-vcc2020/data/f0.yaml"

    fbank_config:
      fs: 24000
      n_mels: 80
      n_fft: 1024
      n_shift: 256
      win_length: null
      window: "hann"
      fmin: 80
      fmax: 7600
      gl_iters: 64

  modelrc:
    ar: True
    hidden_dim: 512
    lstmp_layers: 2
    lstmp_dropout_rate: 0.2
    lstmp_proj_dim: 256
    lstmp_layernorm: False
    dropout_rate: 0.2
